# DocAgent-Arena Benchmark Configuration
# This file drives the entire benchmark orchestration

benchmark:
  # Dataset configuration
  dataset:
    name: qasper                      # Dataset type (qasper, policyqa, etc.)
    split: train                       # train | validation | test
    max_docs: 10                        # Limit for testing (null = all docs)
    max_questions_per_doc: null           # Questions per doc (null = all questions)
    filter_unanswerable: true          # Skip unanswerable questions

  # Provider selection
  # Add/remove providers from this list to change which ones are tested
  providers:
    - llamaindex
    - landingai
    - reducto

  # Execution parameters
  execution:
    # NEW: Parallel task execution (provider, document) combinations
    max_total_workers: 6          # Total parallel (provider, doc) tasks (conservative for 200K TPM)
    max_per_provider_workers: 4   # Max concurrent tasks per provider (rate limiting)
    max_ragas_workers: 2          # Max concurrent RAGAS evaluations (OpenAI rate limit protection)

    # DEPRECATED: Legacy settings (kept for backward compatibility)
    max_provider_workers: 3       # Old: Concurrent providers per document (use max_per_provider_workers instead)
    max_doc_workers: 1            # Old: Documents processed concurrently (now controlled by max_total_workers)

  # Timeout configuration (seconds)
  # Increase these if working with very large documents
  timeouts:
    provider_init: 30
    document_ingest: 300
    query: 60
    evaluation: 120

  # Retry/resilience configuration
  # Future feature: auto-retry failed operations
  retry:
    max_attempts: 3
    delay_seconds: 5
    exponential_backoff: true

  # Output configuration
  output:
    results_dir: data/results         # Base directory for all results
    save_intermediate: true           # Save after each provider completes
    resume_enabled: true              # Skip already-completed documents

  # Ragas evaluation configuration
  evaluation:
    model: gpt-4o-mini                # LLM for evaluation
    api_key_env: OPENAI_API_KEY       # Environment variable for API key
    metrics:
      - faithfulness                  # Is answer grounded in context?
      - factual_correctness           # Does answer match ground truth?
      - context_recall                # Was relevant context retrieved?

  # Provider-specific configurations
  # Each provider has its own initialization parameters
  provider_configs:
    llamaindex:
      top_k: 3                              # Number of chunks to retrieve
      api_key_env: OPENAI_API_KEY           # For embeddings and LLM
      cloud_api_key_env: LLAMAINDEX_API_KEY # For LlamaCloud

    landingai:
      top_k: 3
      api_key_env: VISION_AGENT_API_KEY     # LandingAI API key
      openai_api_key_env: OPENAI_API_KEY    # For embeddings and LLM

    reducto:
      top_k: 3
      api_key_env: REDUCTO_API_KEY          # Reducto API key
      openai_api_key_env: OPENAI_API_KEY    # For embeddings and LLM
